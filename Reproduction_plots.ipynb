{"cells":[{"cell_type":"markdown","metadata":{"id":"yGm4fad3M-Sr"},"source":["# MSWC FSCIL NeuroBench Tutorial\n","\n","This tutorial aims to provide an insight on the MSWC FSCIL NeuroBench task and present how you can use the corresponding NeuroBench harness to benchmark your own models and solutions! In particular we give a tutorial to implement the prototypical network approach to both a convolutional and a recurrent spiking network."]},{"cell_type":"markdown","metadata":{"id":"TFDeiTgMKuzZ"},"source":["## Introduction\n","\n","### About FSCIL (Few-Shot Class-Incremental Learning)\n","\n","Learning new tasks from a small amount of experiences while retaining knowledge of prior tasks is a hallmark of biological intelligence and a long-standing goal of general AI. It is especially a key challenge to endow edge devices with the ability to adapt to their environments and users. This benchmark thus evaluates the capacity of a learning solution to successively incorporate new classes over multiple sessions (class-incremental), with only a handful of samples from the new classes to train with (few-shot). The FSCIL task is a recently established benchmark in the computer vision domain (https://arxiv.org/abs/2004.10956), but it has not yet been adapted to other data modalities.\n","\n","### The MSWC FSCIL NeuroBench Task:\n","Aligning with a neuromorphic interest in temporal data modalities, this benchmark introduces a FSCIL task for streaming audio keyword classification using the large Multilingual Spoken Word Corpus (MSWC) dataset (https://mlcommons.org/datasets/multilingual-spoken-words/). The task is designed to be approached in two phases: pre-training and incremental learning:\n","* First, for pre-training, a set of 100 words spanning 5 base languages (English, German, Catalan, French, Kinyarwanda) with 500 training samples each are made available to train an initial model. We provide here 2 pre-trained models, a convolutional and a recurrent spiking one, both trained with gradient descent on the train samples of the 100 base keywords.\n","\n","* Next, for incremental learning, the model undergoes 10 successive sessions to learn words from 10 new languages (Persian, Spanish, Russian, Welsh, Italian, Basque, Polish, Esparanto, Portuguese, Dutch) in a few-shot learning scenario. Each incremental session adds 10 words of the corresponding session language with only 5 training samples available per word. Here we give a tutorial for the prototypical network solution (https://arxiv.org/abs/1703.05175), as presented in the NeuroBench paper."]},{"cell_type":"markdown","metadata":{"id":"6SZ7D7J4Kuza"},"source":["## Benchmark Task"]},{"cell_type":"markdown","metadata":{"id":"TuTIqyHhKuza"},"source":["Import the modules required for running the benchmark:"]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDA2ceDrOUT1","executionInfo":{"status":"ok","timestamp":1720205268531,"user_tz":-120,"elapsed":442,"user":{"displayName":"MANUPRIYA SINGH","userId":"12067966318852334142"}},"outputId":"ca9480c7-0799-42e1-82b9-d3865f6415c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34m'MSWC_FSCIL Reproduction'\u001b[0m/   \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YY3wfL27Kuzb","outputId":"c4daa268-341e-40c3-f45c-8c776b20faee","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1720204589056,"user_tz":-120,"elapsed":89300,"user":{"displayName":"MANUPRIYA SINGH","userId":"12067966318852334142"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting neurobench\n","  Downloading neurobench-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m685.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting llvmlite<0.41.0,>=0.40.1 (from neurobench)\n","  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numba<0.58.0,>=0.57.1 (from neurobench)\n","  Downloading numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from neurobench) (1.25.2)\n","Collecting snntorch<0.8.0,>=0.7.0 (from neurobench)\n","  Downloading snntorch-0.7.0-py2.py3-none-any.whl (108 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tonic<2.0.0,>=1.4.0 (from neurobench)\n","  Downloading tonic-1.4.3-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from neurobench) (2.3.0+cu121)\n","Requirement already satisfied: torchaudio<3.0.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from neurobench) (2.3.0+cu121)\n","Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from neurobench) (4.66.4)\n","Collecting numpy<2.0.0,>=1.24.3 (from neurobench)\n","  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch<0.8.0,>=0.7.0->neurobench) (2.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch<0.8.0,>=0.7.0->neurobench) (3.7.1)\n","Collecting nir (from snntorch<0.8.0,>=0.7.0->neurobench)\n","  Downloading nir-1.0.4-py3-none-any.whl (18 kB)\n","Collecting nirtorch (from snntorch<0.8.0,>=0.7.0->neurobench)\n","  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from tonic<2.0.0,>=1.4.0->neurobench) (3.9.0)\n","Collecting importRosbag>=1.0.4 (from tonic<2.0.0,>=1.4.0->neurobench)\n","  Downloading importRosbag-1.0.4-py3-none-any.whl (28 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tonic<2.0.0,>=1.4.0->neurobench) (1.11.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from tonic<2.0.0,>=1.4.0->neurobench) (4.12.2)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from tonic<2.0.0,>=1.4.0->neurobench) (0.10.2.post1)\n","Collecting pbr (from tonic<2.0.0,>=1.4.0->neurobench)\n","  Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting expelliarmus (from tonic<2.0.0,>=1.4.0->neurobench)\n","  Downloading expelliarmus-1.1.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.0.1->neurobench)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->neurobench) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.0.1->neurobench)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from importRosbag>=1.0.4->tonic<2.0.0,>=1.4.0->neurobench) (67.7.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.1->neurobench) (2.1.5)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (3.0.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (0.3.7)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->tonic<2.0.0,>=1.4.0->neurobench) (1.0.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch<0.8.0,>=0.7.0->neurobench) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch<0.8.0,>=0.7.0->neurobench) (2024.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.1->neurobench) (1.3.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (4.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (2.31.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch<0.8.0,>=0.7.0->neurobench) (1.16.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->tonic<2.0.0,>=1.4.0->neurobench) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic<2.0.0,>=1.4.0->neurobench) (2024.6.2)\n","Installing collected packages: pbr, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, llvmlite, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, importRosbag, expelliarmus, nvidia-cusolver-cu12, nir, tonic, nirtorch, snntorch, neurobench\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: llvmlite\n","    Found existing installation: llvmlite 0.41.1\n","    Uninstalling llvmlite-0.41.1:\n","      Successfully uninstalled llvmlite-0.41.1\n","  Attempting uninstall: numba\n","    Found existing installation: numba 0.58.1\n","    Uninstalling numba-0.58.1:\n","      Successfully uninstalled numba-0.58.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed expelliarmus-1.1.12 importRosbag-1.0.4 llvmlite-0.40.1 neurobench-1.0.5 nir-1.0.4 nirtorch-1.0 numba-0.57.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pbr-6.0.0 snntorch-0.7.0 tonic-1.4.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"38579368e9c345a5960201c02f3d9ad0"}},"metadata":{}}],"source":["!pip install neurobench"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1_ndnZFKuzc"},"outputs":[],"source":["import copy\n","import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, ConcatDataset\n","\n","from neurobench.benchmarks import Benchmark\n","from neurobench.datasets import MSWC\n","from neurobench.datasets.MSWC_IncrementalLoader import IncrementalFewShot\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"k6mC1MDHKuzc"},"source":["We fix the default settings. Redefine them to your liking:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWvHDV2CKuzd"},"outputs":[],"source":["# data in repo root dir\n","ROOT = \"./data/\"\n","\n","NUM_WORKERS = 8\n","BATCH_SIZE = 256\n","NUM_SHOTS = 5 # How many shots to use for evaluation"]},{"cell_type":"markdown","metadata":{"id":"qUSjWtYJKuzd"},"source":["Select the desired device:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idvPf4MOKuzd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720204612615,"user_tz":-120,"elapsed":2,"user":{"displayName":"MANUPRIYA SINGH","userId":"12067966318852334142"}},"outputId":"6b5a9d4e-8d93-4f03-9de7-d21bc7bcd8fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if device == torch.device(\"cuda\"):\n","    PIN_MEMORY = True\n","else:\n","    PIN_MEMORY = False\n","device"]},{"cell_type":"markdown","metadata":{"id":"2EB47bDMKuzd"},"source":["First, decide if you want to go through the tutorial with the spiking neural network (SNN) or the convolutional one (CNN). Precise this by setting `SPIKING` to True (SNN) or False (CNN)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCs_Hg43Kuze"},"outputs":[],"source":["SPIKING = False"]},{"cell_type":"markdown","metadata":{"id":"oCjtl7QgKuze"},"source":["#### Pre-trained model loading\n","\n","We don't cover the pre-training here as it follows a standard gradient descent and can take quite some time.\n","\n","The pre-training step is nevertheless significant for the FSCIL performance. The models are pre-trained on the MSWC base training subset (in code: `MSWC(root=..., subset=\"base\", procedure=\"training\")`) which has 100 classes with 500 samples per class. The detailed pre-training procedure can be found in the _mswc_fscil.py_ code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f726OUuKuze"},"outputs":[],"source":["MODEL_SAVE_DIR = \"./model_data/\"  #Folder where pre-trained models are stored"]},{"cell_type":"markdown","metadata":{"id":"q3rWwVXWKuze"},"source":["We load the corresponding pre-trained model `mswc_rsnn_proto` (SNN) or `mswc_cnn_proto` (CNN) which are made available directly in the NeuroBench github repo under the `examples/mswc_fscil/model_data/` folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wJJUDxKKuze","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1720204715500,"user_tz":-120,"elapsed":6,"user":{"displayName":"MANUPRIYA SINGH","userId":"12067966318852334142"}},"outputId":"80451558-0022-4697-bd27-e3a1a6f45dc2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './model_data/mswc_cnn_proto'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-fd0d266f5f9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             n_output=200, input_kernel=4, pool_kernel=2, drop=True).to(device)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_cnn_proto\"),\n\u001b[0m\u001b[1;32m     20\u001b[0m                         map_location=device)\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_data/mswc_cnn_proto'"]}],"source":["if SPIKING:\n","    model = SNN(\n","        input_shape=(256, 201, 20),\n","        neuron_type=\"RadLIF\",\n","        layer_sizes=[1024, 1024, 200],\n","        normalization=\"batchnorm\",\n","        dropout=0.1,\n","        bidirectional=False,\n","        use_readout_layer=True,\n","        ).to(device)\n","\n","    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_rsnn_proto\"),\n","                        map_location=device)\n","    model.load_state_dict(state_dict)\n","else:\n","    model = M5(n_input=20, stride=2, n_channel=256,\n","            n_output=200, input_kernel=4, pool_kernel=2, drop=True).to(device)\n","\n","    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_cnn_proto\"),\n","                        map_location=device)\n","    model.load_state_dict(state_dict)"]},{"cell_type":"markdown","metadata":{"id":"qe8z1j5gKuze"},"source":["We display the model below. As you can see:\n","\n","- The CNN model follows the multilayer M5 architecture defined in https://arxiv.org/abs/1610.00087 with a tuned kernel size to match the employed pre-processing.\n","- The SNN model consists of 2 recurrent spiking neuron layers and a linear readout layer, adapted from the sparchSNN library https://github.com/idiap/sparch. The spiking neurons are leaky integrate and fire neurons with an extra adaptive variable to mitigate the impact of average activity. All neuron parameters are trained heterogeneously during pre-training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qdGAn67Kuze"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"z0PFotifKuzf"},"source":["Then, we convert the model to a NeuroBench TorchModel to allow for computational metric benchmarking. This creates hooks to the model activity functions. The neural network itself is now stored in `model.net`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIVwCdoLKuzf"},"outputs":[],"source":["from neurobench.models import TorchModel\n","\n","model = TorchModel(model)"]},{"cell_type":"markdown","metadata":{"id":"VtUIDHkMKuzf"},"source":["For manually defined activation modules, like the adapative LIF neuron used for the SNN model, we need to add this hook manually."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZK58a4-mKuzf"},"outputs":[],"source":["if SPIKING:\n","    model.add_activation_module(RadLIFLayer)"]},{"cell_type":"markdown","metadata":{"id":"vKXK8zHMKuzf"},"source":["### Pre-processing\n","\n","For the proposed solution, we employ a state-of-the-art pre-processing, namely Mel Frequency Cepstral Coefficients (MFCC) to extract relevant frequency-based coefficients. We employ the torchaudio MFCC processor (https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) and tune the hop length to fix the resolution to 200Hz and the number of mel coefficients to 20 for a reasonable number of input channels to the network.\n","\n","For the _spiking_ solution, a delta-encoding is added on top of MFCC to convert the signals to spikes. This is done with the Speech2Spike pipeline (https://dl.acm.org/doi/abs/10.1145/3584954.3584995) that has directly been integrated in NeuroBench. We note that this adds a spiking threshold as an extra parameter. It was fixed to 1 following the Speech2Spikes initial observations here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-EDZdQ8Kuzf"},"outputs":[],"source":["from neurobench.preprocessing import MFCCPreProcessor, S2SPreProcessor\n","\n","n_fft = 512\n","win_length = None\n","hop_length = 240\n","n_mels = 20\n","n_mfcc = 20\n","\n","if SPIKING:\n","    encode = S2SPreProcessor(device, transpose=True)\n","    config_change = {\"sample_rate\": 48000,\n","                     \"hop_length\": 240}\n","    encode.configure(threshold=1.0, **config_change)\n","else:\n","    encode = MFCCPreProcessor(\n","        sample_rate=48000,\n","        n_mfcc=n_mfcc,\n","        melkwargs={\n","            \"n_fft\": n_fft,\n","            \"n_mels\": n_mels,\n","            \"hop_length\": hop_length,\n","            \"mel_scale\": \"htk\",\n","            \"f_min\": 20,\n","            \"f_max\": 4000,\n","        },\n","        device = device\n","    )"]},{"cell_type":"markdown","metadata":{"id":"_JBPr7pSKuzg"},"source":["### Preparation for Prototypical Continual Learning\n","\n","Before we can start using the prototypical network approach for learning incremental classes, we need to align the pre-trained model with this approach. The prototypical network approach (https://arxiv.org/abs/1703.05175) indeed relies on implementing a clustering protocol, based on the pre-trained feature extractor, as a linear readout layer; but this requires all parameters of this readout layer to be defined accordingly. Thus we first redefine the readout layer for the 100 base classes following the prototypical network approach (such that they will align with the incremental classes prototypical readout parameters)."]},{"cell_type":"markdown","metadata":{"id":"mJdnK-BDKuzg"},"source":["Before continuing, we load the base training dataset that is the data available to generate the prototypical representations for the base classes. If the MSWC FSCIL dataset is not already available at `ROOT`, the entire dataset will first be downloaded from _Hugging Face_ at the following address: https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUNRfHrhKuzg"},"outputs":[],"source":["base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\")"]},{"cell_type":"markdown","metadata":{"id":"zRG2bShAKuzg"},"source":["Then we create a dataloader **without shuffling** and with a batch_size of 500, which, following the definition of the dataset, will provide all samples of 1 class at each new batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SGkMTvKKuzg"},"outputs":[],"source":["train_loader = DataLoader(base_train_set, batch_size=500, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"]},{"cell_type":"markdown","metadata":{"id":"qY0IBEU3Kuzg"},"source":["The prototypical readout parameters are defined based on the mean extracted feature $c_k$ from all training sample of the corresponding class $k$, which we get by passing all input samples through the backbone of the pre-trained network (all layers except the readout one). The prototypical weights and biases for class $k$ then are: $W_k = 2c_k, \\ \\ b_k=c_kc_k^T$.\n","\n","To do so, we first define a new readout layer supporting 200 classes (100 base classes + 100 incrementally learned classes) that will replace the pre-trained one:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbVRGfBdKuzg"},"outputs":[],"source":["# Set-up new proto readout layer\n","if SPIKING:\n","    output = model.net.snn[-1].W\n","    proto_out = nn.Linear(output.weight.shape[1], 200, bias=True).to(device)\n","    proto_out.weight.data = output.weight.data\n","else:\n","    output = model.net.output\n","    proto_out = nn.Linear(512, 200, bias=True).to(device)\n","    proto_out.weight.data = output.weight.data"]},{"cell_type":"markdown","metadata":{"id":"-Vjm4MpLKuzh"},"source":["Then we pass through each of the base training classes, get all of the 500 associated sample feature, average them and define the weights and biases accordingly.\n","\n","Just note that for the _spiking_ solution, the features are summed over time and thus the bias is also divided by the number of total timesteps.\n","\n","_Note_: This procedure can take a bit of time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vs_jwd8Kuzh"},"outputs":[],"source":["# Compute prototype weights for base classes\n","\n","for data, target in tqdm(train_loader):\n","    data, target = encode((data.to(device), target.to(device)))\n","    data = data.squeeze()\n","    class_id = target[0]\n","\n","    if SPIKING:\n","        features = data\n","        for layer in model.net.snn[:-1]:\n","            features = layer(features)\n","\n","        mean = torch.sum(features, dim=[0,1])/500\n","        proto_out.weight.data[class_id] = 2*mean\n","        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())/features.shape[1]\n","\n","    else:\n","        features = model.net(data, features_out=True)\n","\n","        mean = torch.sum(features, dim=0)/500\n","        proto_out.weight.data[class_id] = 2*mean\n","        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())\n","\n","    del data\n","    del features\n","    del mean"]},{"cell_type":"markdown","metadata":{"id":"bkywQ_Y1Kuzh"},"source":["Finally we replace the pre-trained readout layer by the newly defined prototypical one:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiaZUgVfKuzi"},"outputs":[],"source":["\n","# Replace pre-trained readout with prototypical layer\n","if SPIKING:\n","    model.net.snn[-1].W = proto_out\n","else:\n","    model.net.output = proto_out\n","\n","del base_train_set\n","del train_loader"]},{"cell_type":"markdown","metadata":{"id":"MuidwZ1NKuzi"},"source":["Next, we test the performance of the prototypical representations on the base test set using a NeuroBench Benchmark:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87cce5c8Kuzi"},"outputs":[],"source":["# Copy model for evaluation\n","eval_model = copy.deepcopy(model)\n","\n","# Get base test set for evaluation\n","base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")\n","test_loader = DataLoader(base_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n","\n","# Put the model in evaluation mode\n","eval_model.net.eval()"]},{"cell_type":"markdown","metadata":{"id":"IbQUNDJPKuzi"},"source":["As NeuroBench Benchmarks encapsulate the whole testing, it requires some pre and post-processors to manipulate data before and aftera network pass. We thus define the following utility functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWl-5T_PKuzi"},"outputs":[],"source":["squeeze = lambda x: (x[0].squeeze(), x[1])\n","out2pred = lambda x: torch.argmax(x, dim=-1)\n","to_device = lambda x: (x[0].to(device), x[1].to(device))"]},{"cell_type":"markdown","metadata":{"id":"S8ADZjIaKuzj"},"source":["We also define a mask function for this evaluation as the network is directly defined with 200 output neurons but we are for now evaluating the performance solely on the 100 base classes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqCQBNV1Kuzj"},"outputs":[],"source":["# Define specific post-processing with masking on the base classes\n","mask = torch.full((200,), float('inf')).to(device)\n","mask[torch.arange(0,100, dtype=int)] = 0\n","out_mask = lambda x: x - mask"]},{"cell_type":"markdown","metadata":{"id":"v57FKkLWKuzj"},"source":["Now we can define the Benchmark object with the desired metrics:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjjAUR7-Kuzj"},"outputs":[],"source":["# Metrics\n","static_metrics = [\"footprint\", \"connection_sparsity\"]\n","workload_metrics = [\"classification_accuracy\", \"activation_sparsity\", \"synaptic_operations\"]\n","\n","# Define benchmark object\n","benchmark_all_test = Benchmark(eval_model, metric_list=[static_metrics, workload_metrics],\n","                               dataloader=test_loader,\n","                               preprocessors=[to_device, encode, squeeze], postprocessors=[])"]},{"cell_type":"markdown","metadata":{"id":"x-113uLkKuzj"},"source":["We now run the Benchmark on the base test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrqG2xKyKuzj"},"outputs":[],"source":["pre_train_results = benchmark_all_test.run(postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n","\n","print(\"Base results:\", pre_train_results)\n","\n","print(f\"The base accuracy is {pre_train_results['classification_accuracy']*100}%\")"]},{"cell_type":"markdown","metadata":{"id":"aHJpxDXcKuzj"},"source":["This is the performance of session 0.\n","\n","Note that the obtained accuracy, after conversion to prototypes, is below the original performance of the pre-trained model. This is a price to pay to allow for the prototypical network to work effectively in the incremental sessions. This could nevertheless still be improved upon, especially for the _spiking_ solution, where the conversion accuracy drop is significant (from 93% to 84%)."]},{"cell_type":"markdown","metadata":{"id":"7Nx9ryIrKuzk"},"source":["### Incremental Learning\n","\n","We can now pursue with the few-shot incremental sessions. New sessions are learned following the prototypical network approach on the corresponding session classes and with the limited number of samples available.\n","\n","We first initialize the FSCIL dataloader. It will generate 10 sessions from a random ordering of the 10 incremental languages. Each session consists of\n","- One `support` _list_ of `NUM_SHOTS` shots, each shot being a tuple of tensors `(X_shot, y_shot)` with one sample for each of the 10 session classes.  \n","- One `query` _dataset_ with all the current and prior incremental session classes and `query_shots` samples per class.\n","- One `query_classes` list that contains each unique incremental class index following their order of appearance.\n","\n","Note that the `support_query_split` is here to define a pre-sampling split between samples available for support and for query in this order. In the proposed set-up, the few-shot dataloader thus fixes the 100 query samples per class from the start and samples 5 shots out of a 100 samples for each incremental class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lo2LWJ90Kuzk"},"outputs":[],"source":["# IncrementalFewShot Dataloader used in incremental mode to generate class-incremental sessions\n","few_shot_dataloader = IncrementalFewShot(k_shot=NUM_SHOTS,\n","                            root = ROOT,\n","                            query_shots=100,\n","                            support_query_split=(100,100))"]},{"cell_type":"markdown","metadata":{"id":"LgxrPWtFKuzk"},"source":["We then run one incremental session learning as an example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHG9UarJKuzk"},"outputs":[],"source":["support, query, query_classes = next(iter(few_shot_dataloader))"]},{"cell_type":"markdown","metadata":{"id":"qwO4G_GpKuzk"},"source":["The support data - which is generated in a shot-by-shot way for universality to different methods - is here concatenated to gather all training samples per class for the prototypical approach:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZefdM4YIKuzk"},"outputs":[],"source":["data = None\n","\n","for X_shot, y_shot in support:\n","    if data is None:\n","        data = X_shot\n","        target = y_shot\n","    else:\n","        data = torch.cat((data,X_shot), 0)\n","        target = torch.cat((target,y_shot), 0)\n","\n","data, target = encode((data.to(device), target.to(device)))\n","data = data.squeeze()\n","\n","new_classes = y_shot.tolist()\n","Nways = len(y_shot) # Number of ways of one batch, should always be 10"]},{"cell_type":"markdown","metadata":{"id":"gSajHKI_Kuzk"},"source":["We then apply the prototypical network approach on the corresponding incremental classes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aR9BhhnxKuzl"},"outputs":[],"source":["if SPIKING:\n","    features = eval_model.net.snn[0](data)\n","    features = eval_model.net.snn[1](features)\n","\n","    for index, class_id in enumerate(new_classes):\n","        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n","        eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n","        eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n","else:\n","    features = eval_model.net(data, features_out=True)\n","\n","    for index, class_id in enumerate(new_classes):\n","        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n","        eval_model.net.output.weight.data[class_id] = 2*mean\n","        eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())"]},{"cell_type":"markdown","metadata":{"id":"P_YH9r9xKuzl"},"source":["Then we evaluate the performance after one FSCIL session. The default FSCIL benchmarking evaluates accuracy on all classes seen so far, including the base classes used for pre-training. To this, we add an evaluation of the performance solely on the incremental few-shot classes, corresponding to only the `query` dataset.\n","\n","Note that the dataloaders used for the benchmarking are actually redefined when running the Benchmark object. This is to be aligned with the general case of multiple sessions (see cell below) as the data to test on changes over sessions in a FSCIL task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDiJeNW0Kuzl"},"outputs":[],"source":["# Define benchmark object for incremental classes\n","benchmark_new_classes = Benchmark(eval_model, metric_list=[[],[\"classification_accuracy\"]],\n","                                  dataloader=None,\n","                                  preprocessors=[to_device, encode, squeeze], postprocessors=[])\n","\n","### Testing phase ###\n","eval_model.net.eval()\n","\n","# Define session dataloaders for query and query + base_test samples\n","query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","\n","full_session_test_set = ConcatDataset([base_test_set, query])\n","full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","\n","# Create a mask function to only consider accuracy on classes presented so far\n","session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes)))\n","mask = torch.full((200,), float('inf')).to(device)\n","mask[session_classes] = 0\n","out_mask = lambda x: x - mask\n","\n","\n","# Run benchmark on query classes only\n","query_results = benchmark_new_classes.run(dataloader = query_loader,\n","                                          postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n","print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n","\n","# Run benchmark to evaluate accuracy of this specific session\n","session_results = benchmark_all_test.run(dataloader = full_session_test_loader,\n","                                         postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n","print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")"]},{"cell_type":"markdown","metadata":{"id":"zOJjJupjKuzl"},"source":["Finally, we can run the full FSCIL setup by looping over the code as presented above for all 10 sessions:\n","\n","_Note_: This can take a bit of time as FSCIL requires for increasingly heavy datasets to be loaded in memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2I-5qIZSKuzl"},"outputs":[],"source":["# variable to store accuracy\n","acc = []\n","acc_new_classes = []\n","\n","# Iteration over incremental sessions\n","for session, (support, query, query_classes) in enumerate(few_shot_dataloader):\n","    print(f\"Session: {session+1}\")\n","\n","    ### Computing new Prototypical Weights ###\n","    data = None\n","\n","    for X_shot, y_shot in support:\n","        if data is None:\n","            data = X_shot\n","            target = y_shot\n","        else:\n","            data = torch.cat((data,X_shot), 0)\n","            target = torch.cat((target,y_shot), 0)\n","\n","    data, target = encode((data.to(device), target.to(device)))\n","    data = data.squeeze()\n","\n","    new_classes = y_shot.tolist()\n","    Nways = len(y_shot) # Number of ways, should always be 10\n","\n","    if SPIKING:\n","        features = eval_model.net.snn[0](data)\n","        features = eval_model.net.snn[1](features)\n","\n","        for index, class_id in enumerate(new_classes):\n","            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n","            eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n","            eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n","    else:\n","        features = eval_model.net(data, features_out=True)\n","\n","        for index, class_id in enumerate(new_classes):\n","            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n","            eval_model.net.output.weight.data[class_id] = 2*mean\n","            eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())\n","\n","    ### Testing phase ###\n","    eval_model.net.eval()\n","\n","    # Define session dataloaders for query and query + base_test samples\n","    query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","\n","    full_session_test_set = ConcatDataset([base_test_set, query])\n","    full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","\n","    # Create a mask function to only consider accuracy on classes presented so far\n","    session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes)))\n","    mask = torch.full((200,), float('inf')).to(device)\n","    mask[session_classes] = 0\n","    out_mask = lambda x: x - mask\n","\n","    # Run benchmark on query classes only\n","    query_results = benchmark_new_classes.run(dataloader = query_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n","    print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n","\n","    # Run benchmark to evaluate accuracy of this specific session\n","    session_results = benchmark_all_test.run(dataloader = full_session_test_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n","    print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")\n","    print(\"Session results:\", session_results)\n","    acc.append(session_results['classification_accuracy'] * 100)\n","    acc_new_classes.append(query_results['classification_accuracy'] * 100)\n","\n","\n","print(f\"Accuracy over all sessions: {acc}\")\n","\n","print(acc)\n","import matplotlib.pyplot as plt\n","plt.plot(acc, color='blue')\n","# plt.plot(acc_given_data_set, color='red')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Session')\n","plt.title('Accuracy over all sessions')\n","\n","plt.show()"]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","acc = [92.75454, 85.0, 87.5, 90.0, 91.0, 93.0, 95.0, 96.0, 97.0, 98.0, 99.0]\n","\n","sns.set(style=\"whitegrid\", context=\"notebook\")\n","\n","sessions = np.arange(0, 10, 1)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(sessions, acc, \"-o\", color=\"#2c7fb8\", linewidth=2, markersize=6, markerfacecolor=\"#f03b20\", markeredgewidth=2, markeredgecolor=\"#2c7fb8\")\n","\n","plt.xticks(sessions)\n","plt.ylim((40, 100))\n","\n","plt.ylabel('Accuracy', fontsize=12)\n","plt.xlabel('Session', fontsize=12)\n","plt.title('All Classes Performance', fontsize=14)\n","\n","sns.despine()\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.tight_layout()\n","\n","plt.savefig(\"outputs/All_Classes_Performance.png\", dpi=300)\n","\n","plt.show()\n"],"metadata":{"id":"e65rLxTBMe_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","sessions = np.arange(1, 11, 1)\n","\n","sns.set(style=\"whitegrid\", context=\"notebook\")\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(sessions, acc_new_classes, \"-o\", color=\"#386cb0\", linewidth=2, markersize=6, markerfacecolor=\"#f03b20\", markeredgewidth=2, markeredgecolor=\"#386cb0\")\n","\n","plt.xticks(sessions)\n","plt.ylim((40, 100))\n","\n","plt.ylabel('Accuracy', fontsize=12)\n","plt.xlabel('Session', fontsize=12)\n","plt.title('New Classes Performance', fontsize=14)\n","\n","sns.despine()\n","plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","plt.tight_layout()\n","\n","plt.savefig(\"outputs/New_Classes_Performance.png\", dpi=300)\n","\n","plt.show()"],"metadata":{"id":"xBtHIbVCNj4k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSoodUS6Kuzl"},"source":["You should obtain a performance within the bounds presented in the results plot below. The shaded area represents $5^{th}$ and $95^{th}$ percentile on 100 runs."]},{"cell_type":"markdown","metadata":{"id":"h8H_LiqXKuzm"},"source":["![title](https://github.com/MANUPRIYASINGH/MSWC_FCIL_DL/blob/main/img/FSCIL_proto_results.png?raw=1)"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/MANUPRIYASINGH/MSWC_FCIL_DL/blob/main/MSWC_tutorial.ipynb","timestamp":1720204449663}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}